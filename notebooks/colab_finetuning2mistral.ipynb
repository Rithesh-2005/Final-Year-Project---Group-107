{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9fc77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992a1ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Assuming you uploaded 'workflow_dataset.jsonl' to your Colab workspace\n",
    "dataset = load_dataset(\"json\", data_files=\"workflow_dataset.jsonl\", split=\"train\")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} training examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea295d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use Mistral-7B as it is excellent at JSON/coding tasks\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Configure 4-bit quantization to fit in Colab's 16GB VRAM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b76811",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply LoRA to specific model layers to train efficiently\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7c44e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./workflow-schema-lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3, # Adjust based on dataset size\n",
    "    max_steps=200,      # Remove or adjust to train on the full dataset\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512, # Keeps memory usage low\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Start Fine-tuning!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84336c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "new_model_name = \"workflow-mistral-finetuned\"\n",
    "trainer.model.save_pretrained(new_model_name)\n",
    "tokenizer.save_pretrained(new_model_name)\n",
    "\n",
    "print(f\"Model successfully saved to {new_model_name}\")\n",
    "\n",
    "# Quick Test\n",
    "prompt = \"<s>[INST] Convert this instruction into a workflow schema: Pull the weekly AWS billing data and send an alert if it exceeds budget. [/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = peft_model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38420ef2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Log in to Hugging Face (this will prompt you for your HF write token)\n",
    "notebook_login()\n",
    "\n",
    "# 2. Define your repository name (change 'your-username' to your actual HF username)\n",
    "hf_repo_name = \"your-username/workflow-orchestrator-mistral\"\n",
    "\n",
    "print(\"Loading PEFT model and merging with base model...\")\n",
    "# 3. Load the model and merge it (requires reloading in 16-bit to merge properly)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./workflow-schema-lora\", # The directory where Trainer saved the checkpoints\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 4. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./workflow-schema-lora\")\n",
    "\n",
    "print(f\"Pushing merged model to Hugging Face Hub: {hf_repo_name}...\")\n",
    "# 5. Push to Hub\n",
    "merged_model.push_to_hub(hf_repo_name)\n",
    "tokenizer.push_to_hub(hf_repo_name)\n",
    "\n",
    "print(\"Upload complete! Your model is now live on Hugging Face.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
